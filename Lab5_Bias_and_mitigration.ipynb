{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1828bbc3-0b2a-4950-95d0-ab66d61da073",
   "metadata": {},
   "source": [
    "# Lab 5 - Bias and mitigation\n",
    "\n",
    "In this lab we will explore the concepts of bias and fairness in developing models by revisiting the Titanic classification example you did in Lab 2. In Lab 2 we stepped through training a decision tree classifier, so that given a new hypothetical passenger we can make a prediction if they would be survive (or not) if they had been on the Titanic at the time the ship sank. Here, we will use a different type of classifier and identify potential biases with a simple metric that you have already come across before (accuracy) and try out some simple mitigation techniques.\n",
    "\n",
    "To complete this lab:\n",
    "1. Follow the instructions running the code when asked.\n",
    "2. Discuss each question with a study partner.\n",
    "3. You should keep notes for your answers in a separate document (you can use [this template for Lab 5](Lab5_answers_template.docx)).\n",
    "4. When completed, show your answers to a teacher so they can provide feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d4a9d-198b-4b35-a6ea-2fd04bcb4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b0dd6-f4ce-4c90-8d20-8b12b1ab337c",
   "metadata": {},
   "source": [
    "Let's look again at the Titanic problem example, but this time we will use a SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5644d-f0af-4a04-8785-a7ab3d12d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "titantic_passengers = pd.read_csv(\"titanic.csv\")\n",
    "titantic_passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784028f8-264c-4838-bf68-882ae7891264",
   "metadata": {},
   "source": [
    "**Q1.** Simply by inspecting the features (columns) and looking at some of the values, what _potential_ biases can you identify? Think about the representation of different groups within the overall passenger list. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5312f-1461-407e-b885-4f1525a0ec1f",
   "metadata": {},
   "source": [
    "Hopefully you identified that there may be several subgroups within the passenger list that might be over- or under-represented. Let's take a closer look by inspecting the distributions of values for various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9130f-1343-49eb-b9d9-808a705b8de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_by_gender = titantic_passengers.groupby('sex').survived.mean()\n",
    "_ = survived_by_gender.plot.bar()\n",
    "survived_by_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d39e41-7d6c-4a08-bbd3-2b4a778c7b62",
   "metadata": {},
   "source": [
    "**Q2.** Looking at the different distributions, do these exhibit bias? If so, is it necessarily a bad thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a988b-e280-4d4f-8151-3ed127139c3d",
   "metadata": {},
   "source": [
    "Hopefully you can see that when you inspect the distributions, rarely would you find the value counts balanced - and perhaps this is not a bad thing. For example, let's say our Titanic data only told us something about the sex of the passengers and in their survival rates showed that 50% of all females and 50% of all males survived, and there were exactly the same number of total men and women on the ship. If that were the case, could we predict anything about survival based on this data? Probably not, since the probabilities are evenly split between sexes, 50/50, and the survival rates are perfectly balanced. A model trained on this data would only be able to pick randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b97c4-af22-4f67-ae20-fbdcb9268d70",
   "metadata": {},
   "source": [
    "However, something that was discussed in the lecture on testing ML was how that sometimes certain groups represented within a dataset may not achieve the same predictive performance as others. The degree of equalness between groups when evaluating the performance of classifiers is called _accuracy parity_.\n",
    "\n",
    "Accuracy parity measures whether the accuracy of the model is consistent across different groups. Disparities in accuracy can signal bias. This is what we will explore further in this lab.\n",
    "\n",
    "Let's train our SVM classifier and inspect the overall accuracy of our survival prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c9e61-a2e2-42e5-91fc-a705c3d6904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titantic_passengers['age_range'] = pd.cut(titantic_passengers.age, [0, 15, 80], labels=['child', 'adult'])\n",
    "titantic_passengers.age = titantic_passengers.age.fillna(titantic_passengers.age.median())\n",
    "titantic_passengers.fare = titantic_passengers.fare.fillna(titantic_passengers.fare.median())\n",
    "titantic_passengers = pd.get_dummies(titantic_passengers, columns=['sex'], drop_first=True)\n",
    "survived_data = titantic_passengers.survived  # save this for training later\n",
    "titantic_passengers = titantic_passengers[['sex_male', 'fare', 'age', 'sibsp']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(titantic_passengers, survived_data, test_size=0.25, random_state=42)\n",
    "print(\"Our training data has {} rows\".format(len(X_train)))\n",
    "print(\"Our test data has {} rows\".format(len(X_test)))\n",
    "classifier = SVC(random_state=88)\n",
    "classifier.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889e4d1-6304-4cc0-99bf-5a4af3a16396",
   "metadata": {},
   "source": [
    "We can get a list of predicted outcomes (survived or not) based on our held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d2265-6d34-4771-a018-291d4ff1b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =  classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e66c4a-856c-4dc2-9c40-932ad6427e34",
   "metadata": {},
   "source": [
    "Then, we can compare the predictions to the actual survival in our held-out test data to count the number of correct predictions that our classifier made. Recall from earlier in the course that the accuracy of a classifier is defined as the proportion of correct predictions made by the model. It can be calculated using the formula:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f3d83-42fc-4fb5-827d-b29031e4dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = y_pred == y_test\n",
    "num_correct_predictions = correct_predictions.sum()  # sums all the True values in the correct_predictions series\n",
    "num_correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69f674-ece7-4e8e-9407-b5042804c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predictions = len( ... )\n",
    "total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac6d1f-8004-4d78-af41-ad90132916c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "... / ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d1a54-c819-4f31-b794-71f0c3b60a37",
   "metadata": {},
   "source": [
    "**Q3:** What is the accuracy of the classifier? Provide your answer as a percentage to two decimal places. (Hint: If you haven't already, you can find the answer by filling in the ellipses above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a91296-8ebe-41b4-a1da-d8a3d4511009",
   "metadata": {},
   "source": [
    "**Q4:** Do you think this accuracy will be the same for different subgroups of kinds of passengers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd85a70-611c-4fbb-87f4-c6cd1d2dd80c",
   "metadata": {},
   "source": [
    "Whatever conclusion you came to in Q4, let's now find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b23794-ca33-40d4-a659-bc17446eecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the male subset from our train and test data\n",
    "X_test_male = X_test[X_test['sex_male'] == 1]\n",
    "y_test_male = y_test[X_test['sex_male'] == 1]\n",
    "\n",
    "y_pred_male = classifier.predict(X_test_male)\n",
    "correct_predictions_male = y_pred_male == y_test_male\n",
    "correct_predictions_male.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0165f-fa33-4ae5-b090-33d8278e4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_female = X_test[X_test['sex_male'] == 0]\n",
    "y_test_female = y_test[X_test['sex_male'] == 0]\n",
    "\n",
    "y_pred_female = classifier.predict(X_test_female)\n",
    "correct_predictions_female = y_pred_female == y_test_female\n",
    "correct_predictions_female.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c54cd8-bb8b-4ece-aed6-d65dfcc41aee",
   "metadata": {},
   "source": [
    "**Q5:** What is the accuracy for the male group?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52049e8f-1c19-4c46-a75e-4d35336db337",
   "metadata": {},
   "source": [
    "**Q6:** What is the accuracy for the female group?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89c31e-b793-46de-9c89-aba6567a5abc",
   "metadata": {},
   "source": [
    "**Q7:** Has accuracy parity been achieved? (i.e. are the accuracies similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b0d7d-a9ba-4e28-8c5d-8b89c8e2e976",
   "metadata": {},
   "source": [
    "One strategy to try mitigate bias within groups is stratification. Stratification, in simple terms, refers to the process of dividing a dataset into different \"strata\" or subgroups, making sure that each subgroup is represented in proportion to its prevalence in the whole dataset. This technique is often used in sampling or when splitting data into training and testing sets for model development.\n",
    "\n",
    "Imagine you have a bowl of mixed nuts containing almonds, cashews, and peanuts in different proportions. If you want to create a smaller sample that represents the whole bowl accurately, you would use stratification. You would take a handful that contains almonds, cashews, and peanuts in the same ratio as they are present in the whole bowl, not just a random handful that might contain only almonds or peanuts.\n",
    "\n",
    "In the context of mitigating bias in machine learning and data analysis:\n",
    "\n",
    "- Representative Sampling: Stratification ensures that each subgroup (e.g., different genders, age groups, income levels) in your dataset is proportionally represented. This is crucial in studies and models where it's important to capture the characteristics of the entire population without overemphasizing or underrepresenting any segment.\n",
    "\n",
    "- Reducing Sampling Bias: Without stratification, there's a risk that your sample or training/test split might not accurately represent the broader population, leading to biased results. For instance, in a medical study, if you randomly select participants without considering age or gender, you might end up with a sample skewed towards a specific age group or gender, which could bias your study results.\n",
    "\n",
    "Luckily for us, scikit-learn provides an option for when we do our test-train split to stratify by a particular feature. In this example, let's try straify on survival outcome, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a96697-3f18-4b48-b2d1-444b236eea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(titantic_passengers, survived_data, test_size=0.25, random_state=42, \n",
    "                                                    stratify=survived_data)\n",
    "print(\"Our training data has {} rows\".format(len(X_train)))\n",
    "print(\"Our test data has {} rows\".format(len(X_test)))\n",
    "\n",
    "# Calculate the weights for each group\n",
    "group_weights = X_train['sex_male'].value_counts(normalize=True).to_dict()\n",
    "weights = X_train['sex_male'].apply(lambda x: 1 / group_weights[x])\n",
    "\n",
    "classifier = SVC(random_state=88)\n",
    "classifier.fit(X_train.values, y_train.values)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40d00e-da31-4b3d-880d-cb9e47abb09c",
   "metadata": {},
   "source": [
    "**Q8:** What is the accuracy of the classifier using stratification on survival? Provide your answer as a percentage to two decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7ae18-246c-4d3b-a19b-a64f75d39589",
   "metadata": {},
   "source": [
    "**Q9:** Did the classifier do better or worse when using stratification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016156c5-91c1-4d6f-8f94-b36d449c77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the male subset from our train and test data\n",
    "X_test_male = X_test[X_test['sex_male'] == 1]\n",
    "y_test_male = y_test[X_test['sex_male'] == 1]\n",
    "\n",
    "y_pred_male = classifier.predict(X_test_male)\n",
    "correct_predictions_male = y_pred_male == y_test_male\n",
    "correct_predictions_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143d33f-63c3-4f5f-980d-6d39fb7f6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_female = X_test[X_test['sex_male'] == 0]\n",
    "y_test_female = y_test[X_test['sex_male'] == 0]\n",
    "\n",
    "y_pred_female = classifier.predict(X_test_female)\n",
    "correct_predictions_female = y_pred_female == y_test_female\n",
    "correct_predictions_female"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0063e69-2239-42c6-9219-aeed4291f243",
   "metadata": {},
   "source": [
    "**Q10:** What is the accuracy for the male group when using the stratified classifer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cb953-e3ae-422d-879a-37e9a1917dbf",
   "metadata": {},
   "source": [
    "**Q11:** What is the accuracy for the female group when using the stratified classifer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d2971-f9d6-4531-96bd-302f2decf5d3",
   "metadata": {},
   "source": [
    "**Q12:** Has accuracy parity been achieved? If not, calculate whether using stratification has improved the accuracy parity. (Hint: Compare the differences between the accuracies, before and after stratification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba301b5-ac3b-4295-9ddd-fb8112fb70dc",
   "metadata": {},
   "source": [
    "You may or may not have found that stratification was effective in this case (note: not all mitigation approaches necessarily work in all cases!). Another popular and simple bias mitigation method when it comes to addressing accuracy parity is _reweighing_.\n",
    "\n",
    "Reweighing, in simple terms, is a technique used in data analysis and machine learning to adjust the importance (or weight) given to different samples (or data points) in a dataset. The main goal is to reduce or mitigate bias, especially in situations where certain groups are underrepresented or overrepresented.\n",
    "\n",
    "To understand reweighing, imagine you're making a fruit salad with apples, oranges, and bananas, but you have a lot more apples than oranges and bananas. To balance the flavors, you might decide to give more importance to each orange and banana you add (since there are fewer of them) and less to each apple. In data analysis, this is like giving more \"weight\" to underrepresented groups in your dataset, so their influence on the analysis or model is proportional to the overrepresented groups.\n",
    "\n",
    "In the context of mitigating bias, reweighing is useful for:\n",
    "\n",
    "- Balancing Representation: It helps in balancing the representation of different groups in your data. For example, if a dataset has more men than women, reweighing can increase the influence of the women's data to balance it out.\n",
    "\n",
    "- Fairer Outcomes: By ensuring that all groups are equally represented, reweighing can lead to fairer outcomes in machine learning models. This is particularly important in applications like hiring, loan approval, and healthcare, where biased data can lead to unfair or discriminatory outcomes.\n",
    "\n",
    "Let's try out reweighing on our Titanic dataset and our SVM classifier. Here, we do two steps:\n",
    "\n",
    "1. Counting Men and Women: First, we look at the data and calculate what percentage of the data is men and what percentage is women. This is like counting how many men and women are in a room to see which group is more common.\n",
    "\n",
    "2. Assigning Weights: Next, we give each person in the dataset a 'weight' based on these percentages. If there are fewer women, each woman gets a higher weight, and if there are fewer men, each man gets a higher weight. Think of it like giving a microphone to people in a group discussion. If there are fewer women speaking, you give them a microphone so their voices are heard as loudly as the men's.\n",
    "\n",
    "For example, consider the following (this data is _not_ taken from the actual Titanic dataset):\n",
    "\n",
    "| Person | sex_male |\n",
    "|--------|----------|\n",
    "| A      | 1        |\n",
    "| B      | 0        |\n",
    "| C      | 1        |\n",
    "| D      | 1        |\n",
    "| E      | 0        |\n",
    "\n",
    "In this dataset, we have 3 males and 2 females.\n",
    "\n",
    "First, we calculate the proportion of each gender in the dataset.\n",
    "\n",
    "- Males (1): 3 out of 5, which is $ \\frac{3}{5} = 0.6 $\n",
    "- Females (0): 2 out of 5, which is $ \\frac{2}{5} = 0.4 $\n",
    "\n",
    "Next, we assign weights to each individual based on these proportions. The weight is the inverse of the proportion for each gender.\n",
    "\n",
    "- Weight for Males: $ \\frac{1}{0.6} \\approx 1.67 $\n",
    "- Weight for Females: $ \\frac{1}{0.4} = 2.5 $\n",
    "\n",
    "Now, each individual in the dataset is assigned a weight based on their gender.\n",
    "\n",
    "| Person | sex_male | Weight  |\n",
    "|--------|----------|---------|\n",
    "| A (M)  | 1        | 1.67    |\n",
    "| B (F)  | 0        | 2.5     |\n",
    "| C (M)  | 1        | 1.67    |\n",
    "| D (M)  | 1        | 1.67    |\n",
    "| E (F)  | 0        | 2.5     |\n",
    "\n",
    "Here, males are given a lower weight because they are more common in the dataset, while females, being less common, receive a higher weight. This balancing act ensures that when analysing the data, both genders have an equal influence overall, despite their differing numbers in the dataset.\n",
    "\n",
    "But you don't need to calculate the weights manually, as we can use Python to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cfb8ae-357a-4067-aa85-7fcb990b77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(titantic_passengers, survived_data, test_size=0.25, random_state=42)\n",
    "print(\"Our training data has {} rows\".format(len(X_train)))\n",
    "print(\"Our test data has {} rows\".format(len(X_test)))\n",
    "\n",
    "# Calculate the weights for each group\n",
    "group_weights = X_train['sex_male'].value_counts(normalize=True).to_dict()\n",
    "weights = X_train['sex_male'].apply(lambda x: 1 / group_weights[x])\n",
    "\n",
    "classifier = SVC(random_state=88)\n",
    "classifier.fit(X_train.values, y_train.values, sample_weight=weights)\n",
    "\n",
    "# Predict and evaluate (assuming you have X_test and y_test)\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85434c7-a565-40ca-8a69-cf91d0721dca",
   "metadata": {},
   "source": [
    "**Q13:** What is the accuracy of the classifier using reweighing on sex? Provide your answer as a percentage to two decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce00e8-cd1c-4db9-ac61-0f5335826a6a",
   "metadata": {},
   "source": [
    "**Q14:** Did the classifier do better or worse when using reweighing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1eeab-33cf-4480-a48e-6c950e0d6a30",
   "metadata": {},
   "source": [
    "Now let's check the accuracy parity again, this time on the classifier trained on the reweighed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690a1a9-ac7c-4007-8652-f3d5ec3b4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217de75-6e95-4ed1-9786-a21ad7124257",
   "metadata": {},
   "source": [
    "**Q15:** What is the accuracy for the male group when using the reweighed classifer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbcc31a-ad94-4e24-a3d7-3e4456115887",
   "metadata": {},
   "source": [
    "**Q16:** What is the accuracy for the female group when using the reweighed classifer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5aea6e-96fd-40da-b076-98b2377cd731",
   "metadata": {},
   "source": [
    "**Q17:** Has accuracy parity been achieved? If not, calculate whether using reweighing has improved the accuracy parity, as compared to the stratification approach and the original model (with no mitigation strategy applied)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac383910-3429-4c62-aada-8ba1fc40df65",
   "metadata": {},
   "source": [
    "In this lab, we show how you can inspect one aspect of bias with a specific metric (accuracy parity) and some very simple approaches to attempting to mitigate any apparent discrepencies that could signal bias. However, there are much more advanced algorithms and tools, such as IBM's AIF360 (https://aif360.res.ibm.com/) and the Fairlearn (https://fairlearn.org/) project that was originally developed by Microsoft, to do this for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c955c9-90a5-4e35-bcf4-0aaaef65551a",
   "metadata": {},
   "source": [
    "**Extra task:** If you're feeling adventurous, try and go back to the start of the notebook and work through it again but choosing a different subset that you might think expresses potential biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549e995-b600-4451-bd9d-384a64a63009",
   "metadata": {},
   "source": [
    "---\n",
    "**When you are finished, let a teacher know you are finished and to check if you have any questions about the lab.**\n",
    "\n",
    "If you wish to save your work in this notebook, choose **Save and Checkpoint** from the **File** menu, then choose **Download as Notebook**  from the **File** menu and save it to your computer or USB stick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc74bc-258f-4969-a5ca-e6a952eab3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
